{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from time import sleep\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import pprint as pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import scipy.stats as sp\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_new_decision_threshold(probability, new_threshold):\n",
    "    if probability > new_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def custom_train_test_split(df, vectorizer, X, y):\n",
    "    '''Takes in original dataframe, vectorizer, sparse matrix of X values, and y values in a Series. Asks if vectorizer \n",
    "        is bigram or not and if there are sentiment scores. Sentiment scores should be stored in the original dataframe as\n",
    "        \"compound score\".\n",
    "        Returns two dataframes for each class (has_scene, no_scene), \n",
    "        train and test for X and y (X_train, X_test, y_train, y_test), \n",
    "        and the train and test dataframes for predictions (train, test).'''\n",
    "    \n",
    "    is_bigram = str(input('Bigram? (Y/N)'))\n",
    "    if is_bigram == 'Y':\n",
    "        pre_split = pd.DataFrame(X.todense(), \n",
    "                                 columns=vectorizer.get_feature_names()).join(pd.DataFrame(y)).join(df.title)\n",
    "        pre_split.rename(columns={'title':'title_of_movie'}, inplace=True)\n",
    "        print('Bigram specified')\n",
    "    else:\n",
    "        print('Non-bigram')\n",
    "        pre_split = pd.DataFrame(X.todense(), \n",
    "                             columns=vectorizer.get_feature_names()).join(pd.DataFrame(y)).join(df.title, \n",
    "                                                                                                  rsuffix='_of_movie')\n",
    "        print('Non-bigram')\n",
    "    \n",
    "    has_compound_score = str(input('Sentiment score? (Y/N)'))\n",
    "    if has_compound_score == 'Y':\n",
    "        print('Has sentiment score.')\n",
    "        pre_split = pre_split.join(df.compound_score)\n",
    "        \n",
    "    pre_split = pre_split.fillna(0)\n",
    "    no_scene_df = pre_split[pre_split.trigger_scene == False]\n",
    "    has_scene_df = pre_split[pre_split.trigger_scene == True]\n",
    "    \n",
    "    n_self_harm = len(has_scene_df.title_of_movie.unique())\n",
    "    n_no_self_harm = len(no_scene_df.title_of_movie.unique())\n",
    "    percent_in_train = 0.7\n",
    "\n",
    "    print(f'Number of movies with self-harm scenes: {n_self_harm}')\n",
    "    print(f'Number of movies with no self-harm scenes: {n_no_self_harm}')\n",
    "\n",
    "    print('----------------------------------------------------------')\n",
    "\n",
    "    n_self_harm_in_train = round(n_self_harm * percent_in_train)\n",
    "    n_no_self_harm_in_train = round(n_no_self_harm * percent_in_train)\n",
    "\n",
    "    print(f'Number of self-harm movies to put into the train set: {n_self_harm_in_train}')\n",
    "    print(f'Number of no self-harm movies to put into the train set: {n_no_self_harm_in_train}')\n",
    "    \n",
    "    # X variables\n",
    "    \n",
    "    last_has_scene_movie_in_train = has_scene_df.title_of_movie.unique()[:n_self_harm_in_train][-1]\n",
    "    index_of_last_has_scene_movie_in_train = (has_scene_df[has_scene_df.title_of_movie == last_has_scene_movie_in_train]\n",
    "                                              .index[-1])\n",
    "\n",
    "    has_scene_rows_to_include_in_train = has_scene_df.loc[:index_of_last_has_scene_movie_in_train].iloc[:,:-3]\n",
    "    has_scene_rows_to_include_in_test = has_scene_df.loc[index_of_last_has_scene_movie_in_train + 1:].iloc[:,:-3]\n",
    "\n",
    "    last_no_scene_movie_in_train = no_scene_df.title_of_movie.unique()[:n_no_self_harm_in_train][-1]\n",
    "    index_of_last_no_scene_movie_in_train = (no_scene_df[no_scene_df.title_of_movie == last_no_scene_movie_in_train]\n",
    "                                              .index[-1])\n",
    "\n",
    "    no_scene_rows_to_include_in_train = no_scene_df.loc[:index_of_last_no_scene_movie_in_train].iloc[:,:-3]\n",
    "    no_scene_rows_to_include_in_test = no_scene_df.loc[index_of_last_no_scene_movie_in_train + 1:].iloc[:,:-3]\n",
    "    \n",
    "    X_train = pd.concat([no_scene_rows_to_include_in_train, has_scene_rows_to_include_in_train])\n",
    "    X_test = pd.concat([no_scene_rows_to_include_in_test, has_scene_rows_to_include_in_test])\n",
    "\n",
    "    print(f'Number of rows in train: {len(X_train)}')\n",
    "    print(f'Number of rows in test: {len(X_test)}')\n",
    "    \n",
    "    if (len(X_train) + len(X_test)) == df.shape[0]:\n",
    "        print('Number of rows match up')\n",
    "    else:\n",
    "        print('Number of rows do not match up')\n",
    "    \n",
    "    # y variable\n",
    "    \n",
    "    has_scene_class_rows_to_include_in_train = has_scene_df.loc[:index_of_last_has_scene_movie_in_train].iloc[:,-3]\n",
    "    has_scene_class_rows_to_include_in_test = has_scene_df.loc[index_of_last_has_scene_movie_in_train + 1:].iloc[:,-3]\n",
    "\n",
    "    no_scene_class_rows_to_include_in_train = no_scene_df.loc[:index_of_last_no_scene_movie_in_train].iloc[:,-3]\n",
    "    no_scene_class_rows_to_include_in_test = no_scene_df.loc[index_of_last_no_scene_movie_in_train + 1:].iloc[:,-3]\n",
    "    \n",
    "    y_train = pd.concat([no_scene_class_rows_to_include_in_train, has_scene_class_rows_to_include_in_train])\n",
    "    y_test = pd.concat([no_scene_class_rows_to_include_in_test, has_scene_class_rows_to_include_in_test])\n",
    "    \n",
    "    print(f'Number of rows in train: {len(y_train)}')\n",
    "    print(f'Number of rows in test: {len(y_test)}')\n",
    "    \n",
    "    \n",
    "    if (len(y_train) + len(y_test)) == df.shape[0]:\n",
    "        print('Number of rows match up')\n",
    "    else:\n",
    "        print('Number of rows do not match up')\n",
    "        \n",
    "    y_train = y_train.astype('bool')\n",
    "    y_test = y_test.astype('bool')\n",
    "    \n",
    "    # movie titles\n",
    "    \n",
    "    has_scene_titles_in_train = has_scene_df.title_of_movie.loc[:index_of_last_has_scene_movie_in_train]\n",
    "    has_scene_titles_in_test = has_scene_df.title_of_movie.loc[index_of_last_has_scene_movie_in_train + 1:]\n",
    "    \n",
    "    no_scene_titles_in_train = no_scene_df.title_of_movie.loc[:index_of_last_no_scene_movie_in_train]\n",
    "    no_scene_titles_in_test = no_scene_df.title_of_movie.loc[index_of_last_no_scene_movie_in_train + 1:]\n",
    "    \n",
    "    titles_train = pd.concat([no_scene_titles_in_train, has_scene_titles_in_train])\n",
    "    titles_test = pd.concat([no_scene_titles_in_test, has_scene_titles_in_test])\n",
    "        \n",
    "    # sentiment scores\n",
    "    \n",
    "    if has_compound_score == 'Y':\n",
    "        has_scene_scores_in_train = has_scene_df.compound_score.loc[:index_of_last_has_scene_movie_in_train]\n",
    "        has_scene_scores_in_test = has_scene_df.compound_score.loc[index_of_last_has_scene_movie_in_train + 1:]\n",
    "\n",
    "        no_scene_scores_in_train = no_scene_df.compound_score.loc[:index_of_last_no_scene_movie_in_train]\n",
    "        no_scene_scores_in_test = no_scene_df.compound_score.loc[index_of_last_no_scene_movie_in_train + 1:]\n",
    "\n",
    "        scores_train = pd.concat([no_scene_scores_in_train, has_scene_scores_in_train])\n",
    "        scores_test = pd.concat([no_scene_scores_in_test, has_scene_scores_in_test])\n",
    "        \n",
    "        X_train = X_train.join(scores_train)\n",
    "        X_test = X_test.join(scores_test)\n",
    "        \n",
    "    # train and test prediction dataframes\n",
    "    \n",
    "    if has_compound_score == 'Y':\n",
    "        train = pd.DataFrame(dict(actual=y_train, title=titles_train, score=scores_train))\n",
    "        test = pd.DataFrame(dict(actual=y_test, title=titles_test, score=scores_test))\n",
    "    else:\n",
    "        train = pd.DataFrame(dict(actual=y_train, title=titles_train))\n",
    "        test = pd.DataFrame(dict(actual=y_test, title=titles_test))\n",
    "    \n",
    "    return no_scene_df, has_scene_df, X_train, X_test, y_train, y_test, train, test\n",
    "\n",
    "def sentiment_categorizer(sentiment_score_dictionary):\n",
    "    compound_score = sentiment_score_dictionary['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score > -0.05 and compound_score < 0.05:\n",
    "        return 'neutral'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('trigger_warning_tweets.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                      0\n",
       "tweet                      1\n",
       "trigger_scene              0\n",
       "cleaned_text               3\n",
       "stemmed_text               3\n",
       "lemmatized_text            3\n",
       "lemmatized_no_stopwords    7\n",
       "stemmed_no_stopwords       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bag of Words sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining features into a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
