{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from time import sleep\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import pprint as pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import scipy.stats as sp\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_new_decision_threshold(probability, new_threshold):\n",
    "    if probability > new_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def custom_train_test_split(df, vectorizer, X, y):\n",
    "    '''Takes in original dataframe, vectorizer, sparse matrix of X values, and y values in a Series. Asks if vectorizer \n",
    "        is bigram or not and if there are sentiment scores. Sentiment scores should be stored in the original dataframe as\n",
    "        \"compound score\".\n",
    "        Returns two dataframes for each class (has_scene, no_scene), \n",
    "        train and test for X and y (X_train, X_test, y_train, y_test), \n",
    "        and the train and test dataframes for predictions (train, test).'''\n",
    "    \n",
    "    is_bigram = str(input('Bigram? (Y/N)'))\n",
    "    if is_bigram == 'Y':\n",
    "        pre_split = pd.DataFrame(X.todense(), \n",
    "                                 columns=vectorizer.get_feature_names()).join(pd.DataFrame(y)).join(df.title)\n",
    "        pre_split.rename(columns={'title':'title_of_movie'}, inplace=True)\n",
    "        print('Bigram specified')\n",
    "    else:\n",
    "        print('Non-bigram')\n",
    "        pre_split = pd.DataFrame(X.todense(), \n",
    "                             columns=vectorizer.get_feature_names()).join(pd.DataFrame(y)).join(df.title, \n",
    "                                                                                                  rsuffix='_of_movie')\n",
    "        print('Non-bigram')\n",
    "    \n",
    "    has_compound_score = str(input('Sentiment score? (Y/N)'))\n",
    "    if has_compound_score == 'Y':\n",
    "        print('Has sentiment score.')\n",
    "        pre_split = pre_split.join(df.compound_score)\n",
    "        \n",
    "    pre_split = pre_split.fillna(0)\n",
    "    no_scene_df = pre_split[pre_split.trigger_scene == False]\n",
    "    has_scene_df = pre_split[pre_split.trigger_scene == True]\n",
    "    \n",
    "    n_self_harm = len(has_scene_df.title_of_movie.unique())\n",
    "    n_no_self_harm = len(no_scene_df.title_of_movie.unique())\n",
    "    percent_in_train = 0.7\n",
    "\n",
    "    print(f'Number of movies with self-harm scenes: {n_self_harm}')\n",
    "    print(f'Number of movies with no self-harm scenes: {n_no_self_harm}')\n",
    "\n",
    "    print('----------------------------------------------------------')\n",
    "\n",
    "    n_self_harm_in_train = round(n_self_harm * percent_in_train)\n",
    "    n_no_self_harm_in_train = round(n_no_self_harm * percent_in_train)\n",
    "\n",
    "    print(f'Number of self-harm movies to put into the train set: {n_self_harm_in_train}')\n",
    "    print(f'Number of no self-harm movies to put into the train set: {n_no_self_harm_in_train}')\n",
    "    \n",
    "    # X variables\n",
    "    \n",
    "    last_has_scene_movie_in_train = has_scene_df.title_of_movie.unique()[:n_self_harm_in_train][-1]\n",
    "    index_of_last_has_scene_movie_in_train = (has_scene_df[has_scene_df.title_of_movie == last_has_scene_movie_in_train]\n",
    "                                              .index[-1])\n",
    "\n",
    "    has_scene_rows_to_include_in_train = has_scene_df.loc[:index_of_last_has_scene_movie_in_train].iloc[:,:-3]\n",
    "    has_scene_rows_to_include_in_test = has_scene_df.loc[index_of_last_has_scene_movie_in_train + 1:].iloc[:,:-3]\n",
    "\n",
    "    last_no_scene_movie_in_train = no_scene_df.title_of_movie.unique()[:n_no_self_harm_in_train][-1]\n",
    "    index_of_last_no_scene_movie_in_train = (no_scene_df[no_scene_df.title_of_movie == last_no_scene_movie_in_train]\n",
    "                                              .index[-1])\n",
    "\n",
    "    no_scene_rows_to_include_in_train = no_scene_df.loc[:index_of_last_no_scene_movie_in_train].iloc[:,:-3]\n",
    "    no_scene_rows_to_include_in_test = no_scene_df.loc[index_of_last_no_scene_movie_in_train + 1:].iloc[:,:-3]\n",
    "    \n",
    "    X_train = pd.concat([no_scene_rows_to_include_in_train, has_scene_rows_to_include_in_train])\n",
    "    X_test = pd.concat([no_scene_rows_to_include_in_test, has_scene_rows_to_include_in_test])\n",
    "\n",
    "    print(f'Number of rows in train: {len(X_train)}')\n",
    "    print(f'Number of rows in test: {len(X_test)}')\n",
    "    \n",
    "    if (len(X_train) + len(X_test)) == df.shape[0]:\n",
    "        print('Number of rows match up')\n",
    "    else:\n",
    "        print('Number of rows do not match up')\n",
    "    \n",
    "    # y variable\n",
    "    \n",
    "    has_scene_class_rows_to_include_in_train = has_scene_df.loc[:index_of_last_has_scene_movie_in_train].iloc[:,-3]\n",
    "    has_scene_class_rows_to_include_in_test = has_scene_df.loc[index_of_last_has_scene_movie_in_train + 1:].iloc[:,-3]\n",
    "\n",
    "    no_scene_class_rows_to_include_in_train = no_scene_df.loc[:index_of_last_no_scene_movie_in_train].iloc[:,-3]\n",
    "    no_scene_class_rows_to_include_in_test = no_scene_df.loc[index_of_last_no_scene_movie_in_train + 1:].iloc[:,-3]\n",
    "    \n",
    "    y_train = pd.concat([no_scene_class_rows_to_include_in_train, has_scene_class_rows_to_include_in_train])\n",
    "    y_test = pd.concat([no_scene_class_rows_to_include_in_test, has_scene_class_rows_to_include_in_test])\n",
    "    \n",
    "    print(f'Number of rows in train: {len(y_train)}')\n",
    "    print(f'Number of rows in test: {len(y_test)}')\n",
    "    \n",
    "    \n",
    "    if (len(y_train) + len(y_test)) == df.shape[0]:\n",
    "        print('Number of rows match up')\n",
    "    else:\n",
    "        print('Number of rows do not match up')\n",
    "        \n",
    "    y_train = y_train.astype('bool')\n",
    "    y_test = y_test.astype('bool')\n",
    "    \n",
    "    # movie titles\n",
    "    \n",
    "    has_scene_titles_in_train = has_scene_df.title_of_movie.loc[:index_of_last_has_scene_movie_in_train]\n",
    "    has_scene_titles_in_test = has_scene_df.title_of_movie.loc[index_of_last_has_scene_movie_in_train + 1:]\n",
    "    \n",
    "    no_scene_titles_in_train = no_scene_df.title_of_movie.loc[:index_of_last_no_scene_movie_in_train]\n",
    "    no_scene_titles_in_test = no_scene_df.title_of_movie.loc[index_of_last_no_scene_movie_in_train + 1:]\n",
    "    \n",
    "    titles_train = pd.concat([no_scene_titles_in_train, has_scene_titles_in_train])\n",
    "    titles_test = pd.concat([no_scene_titles_in_test, has_scene_titles_in_test])\n",
    "        \n",
    "    # sentiment scores\n",
    "    \n",
    "    if has_compound_score == 'Y':\n",
    "        has_scene_scores_in_train = has_scene_df.compound_score.loc[:index_of_last_has_scene_movie_in_train]\n",
    "        has_scene_scores_in_test = has_scene_df.compound_score.loc[index_of_last_has_scene_movie_in_train + 1:]\n",
    "\n",
    "        no_scene_scores_in_train = no_scene_df.compound_score.loc[:index_of_last_no_scene_movie_in_train]\n",
    "        no_scene_scores_in_test = no_scene_df.compound_score.loc[index_of_last_no_scene_movie_in_train + 1:]\n",
    "\n",
    "        scores_train = pd.concat([no_scene_scores_in_train, has_scene_scores_in_train])\n",
    "        scores_test = pd.concat([no_scene_scores_in_test, has_scene_scores_in_test])\n",
    "        \n",
    "        X_train = X_train.join(scores_train)\n",
    "        X_test = X_test.join(scores_test)\n",
    "        \n",
    "    # train and test prediction dataframes\n",
    "    \n",
    "    if has_compound_score == 'Y':\n",
    "        train = pd.DataFrame(dict(actual=y_train, title=titles_train, score=scores_train))\n",
    "        test = pd.DataFrame(dict(actual=y_test, title=titles_test, score=scores_test))\n",
    "    else:\n",
    "        train = pd.DataFrame(dict(actual=y_train, title=titles_train))\n",
    "        test = pd.DataFrame(dict(actual=y_test, title=titles_test))\n",
    "    \n",
    "    return no_scene_df, has_scene_df, X_train, X_test, y_train, y_test, train, test\n",
    "\n",
    "def sentiment_categorizer(sentiment_score_dictionary):\n",
    "    compound_score = sentiment_score_dictionary['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score > -0.05 and compound_score < 0.05:\n",
    "        return 'neutral'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tweet</th>\n",
       "      <th>trigger_scene</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>lemmatized_no_stopwords</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>False</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>joke peter mistake acdc led zeppelin triggered...</td>\n",
       "      <td>joke peter mistak acdc led zeppelin ptsd becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>Trigger warning for all photographers before s...</td>\n",
       "      <td>False</td>\n",
       "      <td>trigger warning for all photographers before s...</td>\n",
       "      <td>trigger warn for all photograph befor see spid...</td>\n",
       "      <td>trigger warning for all photographer before se...</td>\n",
       "      <td>warning photographer seeing</td>\n",
       "      <td>warn photograph befor see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>False</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>so i just finish watch spiderman far from home...</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>finished loved im car hearing fever got confus...</td>\n",
       "      <td>finish watch im car hear fever got confus bc t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>Spiderman: Far From Home was a gaslighting tri...</td>\n",
       "      <td>False</td>\n",
       "      <td>spiderman far from home was a gaslighting trig...</td>\n",
       "      <td>spiderman far from home wa a gaslight trigger ...</td>\n",
       "      <td>spiderman far from home wa a gaslighting trigg...</td>\n",
       "      <td>wa gaslighting half aint nobody warned</td>\n",
       "      <td>wa gaslight half aint nobodi warn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>it trigger me every time there's a spiderman f...</td>\n",
       "      <td>False</td>\n",
       "      <td>it trigger me every time theres a spiderman fa...</td>\n",
       "      <td>it trigger me everi time there a spiderman far...</td>\n",
       "      <td>it trigger me every time there a spiderman far...</td>\n",
       "      <td>every trailer tv start nowhere playing tom hol...</td>\n",
       "      <td>everi trailer tv start nowher tom holland cri ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                              tweet  \\\n",
       "0  spiderman_far_from_home  spiderman far from home had a joke where peter...   \n",
       "1  spiderman_far_from_home  Trigger warning for all photographers before s...   \n",
       "2  spiderman_far_from_home  so i just finished watching spiderman far from...   \n",
       "3  spiderman_far_from_home  Spiderman: Far From Home was a gaslighting tri...   \n",
       "4  spiderman_far_from_home  it trigger me every time there's a spiderman f...   \n",
       "\n",
       "   trigger_scene                                       cleaned_text  \\\n",
       "0          False  spiderman far from home had a joke where peter...   \n",
       "1          False  trigger warning for all photographers before s...   \n",
       "2          False  so i just finished watching spiderman far from...   \n",
       "3          False  spiderman far from home was a gaslighting trig...   \n",
       "4          False  it trigger me every time theres a spiderman fa...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  spiderman far from home had a joke where peter...   \n",
       "1  trigger warn for all photograph befor see spid...   \n",
       "2  so i just finish watch spiderman far from home...   \n",
       "3  spiderman far from home wa a gaslight trigger ...   \n",
       "4  it trigger me everi time there a spiderman far...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  spiderman far from home had a joke where peter...   \n",
       "1  trigger warning for all photographer before se...   \n",
       "2  so i just finished watching spiderman far from...   \n",
       "3  spiderman far from home wa a gaslighting trigg...   \n",
       "4  it trigger me every time there a spiderman far...   \n",
       "\n",
       "                             lemmatized_no_stopwords  \\\n",
       "0  joke peter mistake acdc led zeppelin triggered...   \n",
       "1                        warning photographer seeing   \n",
       "2  finished loved im car hearing fever got confus...   \n",
       "3             wa gaslighting half aint nobody warned   \n",
       "4  every trailer tv start nowhere playing tom hol...   \n",
       "\n",
       "                                stemmed_no_stopwords  \n",
       "0  joke peter mistak acdc led zeppelin ptsd becau...  \n",
       "1                          warn photograph befor see  \n",
       "2  finish watch im car hear fever got confus bc t...  \n",
       "3                  wa gaslight half aint nobodi warn  \n",
       "4  everi trailer tv start nowher tom holland cri ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('trigger_warning_tweets.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                      0\n",
       "tweet                      1\n",
       "trigger_scene              0\n",
       "cleaned_text               3\n",
       "stemmed_text               3\n",
       "lemmatized_text            3\n",
       "lemmatized_no_stopwords    7\n",
       "stemmed_no_stopwords       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                      0\n",
       "tweet                      0\n",
       "trigger_scene              0\n",
       "cleaned_text               0\n",
       "stemmed_text               0\n",
       "lemmatized_text            0\n",
       "lemmatized_no_stopwords    0\n",
       "stemmed_no_stopwords       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sentiment Score Using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tweet</th>\n",
       "      <th>trigger_scene</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>lemmatized_no_stopwords</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>sentiment_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>False</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>joke peter mistake acdc led zeppelin triggered...</td>\n",
       "      <td>joke peter mistak acdc led zeppelin ptsd becau...</td>\n",
       "      <td>{'neg': 0.173, 'neu': 0.781, 'pos': 0.046, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>Trigger warning for all photographers before s...</td>\n",
       "      <td>False</td>\n",
       "      <td>trigger warning for all photographers before s...</td>\n",
       "      <td>trigger warn for all photograph befor see spid...</td>\n",
       "      <td>trigger warning for all photographer before se...</td>\n",
       "      <td>warning photographer seeing</td>\n",
       "      <td>warn photograph befor see</td>\n",
       "      <td>{'neg': 0.194, 'neu': 0.806, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>False</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>so i just finish watch spiderman far from home...</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>finished loved im car hearing fever got confus...</td>\n",
       "      <td>finish watch im car hear fever got confus bc t...</td>\n",
       "      <td>{'neg': 0.062, 'neu': 0.886, 'pos': 0.052, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>Spiderman: Far From Home was a gaslighting tri...</td>\n",
       "      <td>False</td>\n",
       "      <td>spiderman far from home was a gaslighting trig...</td>\n",
       "      <td>spiderman far from home wa a gaslight trigger ...</td>\n",
       "      <td>spiderman far from home wa a gaslighting trigg...</td>\n",
       "      <td>wa gaslighting half aint nobody warned</td>\n",
       "      <td>wa gaslight half aint nobodi warn</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>it trigger me every time there's a spiderman f...</td>\n",
       "      <td>False</td>\n",
       "      <td>it trigger me every time theres a spiderman fa...</td>\n",
       "      <td>it trigger me everi time there a spiderman far...</td>\n",
       "      <td>it trigger me every time there a spiderman far...</td>\n",
       "      <td>every trailer tv start nowhere playing tom hol...</td>\n",
       "      <td>everi trailer tv start nowher tom holland cri ...</td>\n",
       "      <td>{'neg': 0.19, 'neu': 0.81, 'pos': 0.0, 'compou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                              tweet  \\\n",
       "0  spiderman_far_from_home  spiderman far from home had a joke where peter...   \n",
       "1  spiderman_far_from_home  Trigger warning for all photographers before s...   \n",
       "2  spiderman_far_from_home  so i just finished watching spiderman far from...   \n",
       "3  spiderman_far_from_home  Spiderman: Far From Home was a gaslighting tri...   \n",
       "4  spiderman_far_from_home  it trigger me every time there's a spiderman f...   \n",
       "\n",
       "   trigger_scene                                       cleaned_text  \\\n",
       "0          False  spiderman far from home had a joke where peter...   \n",
       "1          False  trigger warning for all photographers before s...   \n",
       "2          False  so i just finished watching spiderman far from...   \n",
       "3          False  spiderman far from home was a gaslighting trig...   \n",
       "4          False  it trigger me every time theres a spiderman fa...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  spiderman far from home had a joke where peter...   \n",
       "1  trigger warn for all photograph befor see spid...   \n",
       "2  so i just finish watch spiderman far from home...   \n",
       "3  spiderman far from home wa a gaslight trigger ...   \n",
       "4  it trigger me everi time there a spiderman far...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  spiderman far from home had a joke where peter...   \n",
       "1  trigger warning for all photographer before se...   \n",
       "2  so i just finished watching spiderman far from...   \n",
       "3  spiderman far from home wa a gaslighting trigg...   \n",
       "4  it trigger me every time there a spiderman far...   \n",
       "\n",
       "                             lemmatized_no_stopwords  \\\n",
       "0  joke peter mistake acdc led zeppelin triggered...   \n",
       "1                        warning photographer seeing   \n",
       "2  finished loved im car hearing fever got confus...   \n",
       "3             wa gaslighting half aint nobody warned   \n",
       "4  every trailer tv start nowhere playing tom hol...   \n",
       "\n",
       "                                stemmed_no_stopwords  \\\n",
       "0  joke peter mistak acdc led zeppelin ptsd becau...   \n",
       "1                          warn photograph befor see   \n",
       "2  finish watch im car hear fever got confus bc t...   \n",
       "3                  wa gaslight half aint nobodi warn   \n",
       "4  everi trailer tv start nowher tom holland cri ...   \n",
       "\n",
       "                                    sentiment_scores  \n",
       "0  {'neg': 0.173, 'neu': 0.781, 'pos': 0.046, 'co...  \n",
       "1  {'neg': 0.194, 'neu': 0.806, 'pos': 0.0, 'comp...  \n",
       "2  {'neg': 0.062, 'neu': 0.886, 'pos': 0.052, 'co...  \n",
       "3  {'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'comp...  \n",
       "4  {'neg': 0.19, 'neu': 0.81, 'pos': 0.0, 'compou...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_scores'] = df.lemmatized_text.apply(analyzer.polarity_scores)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tweet</th>\n",
       "      <th>trigger_scene</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>lemmatized_no_stopwords</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>compound_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>False</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>joke peter mistake acdc led zeppelin triggered...</td>\n",
       "      <td>joke peter mistak acdc led zeppelin ptsd becau...</td>\n",
       "      <td>{'neg': 0.173, 'neu': 0.781, 'pos': 0.046, 'co...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>Trigger warning for all photographers before s...</td>\n",
       "      <td>False</td>\n",
       "      <td>trigger warning for all photographers before s...</td>\n",
       "      <td>trigger warn for all photograph befor see spid...</td>\n",
       "      <td>trigger warning for all photographer before se...</td>\n",
       "      <td>warning photographer seeing</td>\n",
       "      <td>warn photograph befor see</td>\n",
       "      <td>{'neg': 0.194, 'neu': 0.806, 'pos': 0.0, 'comp...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>False</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>so i just finish watch spiderman far from home...</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>finished loved im car hearing fever got confus...</td>\n",
       "      <td>finish watch im car hear fever got confus bc t...</td>\n",
       "      <td>{'neg': 0.062, 'neu': 0.886, 'pos': 0.052, 'co...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>Spiderman: Far From Home was a gaslighting tri...</td>\n",
       "      <td>False</td>\n",
       "      <td>spiderman far from home was a gaslighting trig...</td>\n",
       "      <td>spiderman far from home wa a gaslight trigger ...</td>\n",
       "      <td>spiderman far from home wa a gaslighting trigg...</td>\n",
       "      <td>wa gaslighting half aint nobody warned</td>\n",
       "      <td>wa gaslight half aint nobodi warn</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'comp...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>it trigger me every time there's a spiderman f...</td>\n",
       "      <td>False</td>\n",
       "      <td>it trigger me every time theres a spiderman fa...</td>\n",
       "      <td>it trigger me everi time there a spiderman far...</td>\n",
       "      <td>it trigger me every time there a spiderman far...</td>\n",
       "      <td>every trailer tv start nowhere playing tom hol...</td>\n",
       "      <td>everi trailer tv start nowher tom holland cri ...</td>\n",
       "      <td>{'neg': 0.19, 'neu': 0.81, 'pos': 0.0, 'compou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                              tweet  \\\n",
       "0  spiderman_far_from_home  spiderman far from home had a joke where peter...   \n",
       "1  spiderman_far_from_home  Trigger warning for all photographers before s...   \n",
       "2  spiderman_far_from_home  so i just finished watching spiderman far from...   \n",
       "3  spiderman_far_from_home  Spiderman: Far From Home was a gaslighting tri...   \n",
       "4  spiderman_far_from_home  it trigger me every time there's a spiderman f...   \n",
       "\n",
       "   trigger_scene                                       cleaned_text  \\\n",
       "0          False  spiderman far from home had a joke where peter...   \n",
       "1          False  trigger warning for all photographers before s...   \n",
       "2          False  so i just finished watching spiderman far from...   \n",
       "3          False  spiderman far from home was a gaslighting trig...   \n",
       "4          False  it trigger me every time theres a spiderman fa...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  spiderman far from home had a joke where peter...   \n",
       "1  trigger warn for all photograph befor see spid...   \n",
       "2  so i just finish watch spiderman far from home...   \n",
       "3  spiderman far from home wa a gaslight trigger ...   \n",
       "4  it trigger me everi time there a spiderman far...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  spiderman far from home had a joke where peter...   \n",
       "1  trigger warning for all photographer before se...   \n",
       "2  so i just finished watching spiderman far from...   \n",
       "3  spiderman far from home wa a gaslighting trigg...   \n",
       "4  it trigger me every time there a spiderman far...   \n",
       "\n",
       "                             lemmatized_no_stopwords  \\\n",
       "0  joke peter mistake acdc led zeppelin triggered...   \n",
       "1                        warning photographer seeing   \n",
       "2  finished loved im car hearing fever got confus...   \n",
       "3             wa gaslighting half aint nobody warned   \n",
       "4  every trailer tv start nowhere playing tom hol...   \n",
       "\n",
       "                                stemmed_no_stopwords  \\\n",
       "0  joke peter mistak acdc led zeppelin ptsd becau...   \n",
       "1                          warn photograph befor see   \n",
       "2  finish watch im car hear fever got confus bc t...   \n",
       "3                  wa gaslight half aint nobodi warn   \n",
       "4  everi trailer tv start nowher tom holland cri ...   \n",
       "\n",
       "                                    sentiment_scores compound_score  \n",
       "0  {'neg': 0.173, 'neu': 0.781, 'pos': 0.046, 'co...       negative  \n",
       "1  {'neg': 0.194, 'neu': 0.806, 'pos': 0.0, 'comp...       negative  \n",
       "2  {'neg': 0.062, 'neu': 0.886, 'pos': 0.052, 'co...       negative  \n",
       "3  {'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'comp...       positive  \n",
       "4  {'neg': 0.19, 'neu': 0.81, 'pos': 0.0, 'compou...       negative  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['compound_score'] = df.sentiment_scores.apply(sentiment_categorizer)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding the sentiment score categories.\n",
    "0 = negative\n",
    "1 = neutral\n",
    "2 = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mz211\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\mz211\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[['compound_score']])\n",
    "df.compound_score = encoder.transform(df[['compound_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tweet</th>\n",
       "      <th>trigger_scene</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>lemmatized_no_stopwords</th>\n",
       "      <th>stemmed_no_stopwords</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>compound_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>False</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>spiderman far from home had a joke where peter...</td>\n",
       "      <td>joke peter mistake acdc led zeppelin triggered...</td>\n",
       "      <td>joke peter mistak acdc led zeppelin ptsd becau...</td>\n",
       "      <td>{'neg': 0.173, 'neu': 0.781, 'pos': 0.046, 'co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>Trigger warning for all photographers before s...</td>\n",
       "      <td>False</td>\n",
       "      <td>trigger warning for all photographers before s...</td>\n",
       "      <td>trigger warn for all photograph befor see spid...</td>\n",
       "      <td>trigger warning for all photographer before se...</td>\n",
       "      <td>warning photographer seeing</td>\n",
       "      <td>warn photograph befor see</td>\n",
       "      <td>{'neg': 0.194, 'neu': 0.806, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>False</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>so i just finish watch spiderman far from home...</td>\n",
       "      <td>so i just finished watching spiderman far from...</td>\n",
       "      <td>finished loved im car hearing fever got confus...</td>\n",
       "      <td>finish watch im car hear fever got confus bc t...</td>\n",
       "      <td>{'neg': 0.062, 'neu': 0.886, 'pos': 0.052, 'co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>Spiderman: Far From Home was a gaslighting tri...</td>\n",
       "      <td>False</td>\n",
       "      <td>spiderman far from home was a gaslighting trig...</td>\n",
       "      <td>spiderman far from home wa a gaslight trigger ...</td>\n",
       "      <td>spiderman far from home wa a gaslighting trigg...</td>\n",
       "      <td>wa gaslighting half aint nobody warned</td>\n",
       "      <td>wa gaslight half aint nobodi warn</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'comp...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>it trigger me every time there's a spiderman f...</td>\n",
       "      <td>False</td>\n",
       "      <td>it trigger me every time theres a spiderman fa...</td>\n",
       "      <td>it trigger me everi time there a spiderman far...</td>\n",
       "      <td>it trigger me every time there a spiderman far...</td>\n",
       "      <td>every trailer tv start nowhere playing tom hol...</td>\n",
       "      <td>everi trailer tv start nowher tom holland cri ...</td>\n",
       "      <td>{'neg': 0.19, 'neu': 0.81, 'pos': 0.0, 'compou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                              tweet  \\\n",
       "0  spiderman_far_from_home  spiderman far from home had a joke where peter...   \n",
       "1  spiderman_far_from_home  Trigger warning for all photographers before s...   \n",
       "2  spiderman_far_from_home  so i just finished watching spiderman far from...   \n",
       "3  spiderman_far_from_home  Spiderman: Far From Home was a gaslighting tri...   \n",
       "4  spiderman_far_from_home  it trigger me every time there's a spiderman f...   \n",
       "\n",
       "   trigger_scene                                       cleaned_text  \\\n",
       "0          False  spiderman far from home had a joke where peter...   \n",
       "1          False  trigger warning for all photographers before s...   \n",
       "2          False  so i just finished watching spiderman far from...   \n",
       "3          False  spiderman far from home was a gaslighting trig...   \n",
       "4          False  it trigger me every time theres a spiderman fa...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  spiderman far from home had a joke where peter...   \n",
       "1  trigger warn for all photograph befor see spid...   \n",
       "2  so i just finish watch spiderman far from home...   \n",
       "3  spiderman far from home wa a gaslight trigger ...   \n",
       "4  it trigger me everi time there a spiderman far...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  spiderman far from home had a joke where peter...   \n",
       "1  trigger warning for all photographer before se...   \n",
       "2  so i just finished watching spiderman far from...   \n",
       "3  spiderman far from home wa a gaslighting trigg...   \n",
       "4  it trigger me every time there a spiderman far...   \n",
       "\n",
       "                             lemmatized_no_stopwords  \\\n",
       "0  joke peter mistake acdc led zeppelin triggered...   \n",
       "1                        warning photographer seeing   \n",
       "2  finished loved im car hearing fever got confus...   \n",
       "3             wa gaslighting half aint nobody warned   \n",
       "4  every trailer tv start nowhere playing tom hol...   \n",
       "\n",
       "                                stemmed_no_stopwords  \\\n",
       "0  joke peter mistak acdc led zeppelin ptsd becau...   \n",
       "1                          warn photograph befor see   \n",
       "2  finish watch im car hear fever got confus bc t...   \n",
       "3                  wa gaslight half aint nobodi warn   \n",
       "4  everi trailer tv start nowher tom holland cri ...   \n",
       "\n",
       "                                    sentiment_scores  compound_score  \n",
       "0  {'neg': 0.173, 'neu': 0.781, 'pos': 0.046, 'co...               0  \n",
       "1  {'neg': 0.194, 'neu': 0.806, 'pos': 0.0, 'comp...               0  \n",
       "2  {'neg': 0.062, 'neu': 0.886, 'pos': 0.052, 'co...               0  \n",
       "3  {'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'comp...               2  \n",
       "4  {'neg': 0.19, 'neu': 0.81, 'pos': 0.0, 'compou...               0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bag of Words Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_bigrams = CountVectorizer(ngram_range=(2,2))\n",
    "X = bag_of_bigrams.fit_transform(df.lemmatized_no_stopwords)\n",
    "y = df.trigger_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram? (Y/N)Y\n",
      "Bigram specified\n",
      "Sentiment score? (Y/N)Y\n",
      "Has sentiment score.\n",
      "Number of movies with self-harm scenes: 129\n",
      "Number of movies with no self-harm scenes: 83\n",
      "----------------------------------------------------------\n",
      "Number of self-harm movies to put into the train set: 90\n",
      "Number of no self-harm movies to put into the train set: 58\n",
      "Number of rows in train: 2033\n",
      "Number of rows in test: 850\n",
      "Number of rows match up\n",
      "Number of rows in train: 2033\n",
      "Number of rows in test: 850\n",
      "Number of rows match up\n"
     ]
    }
   ],
   "source": [
    "has_scene_df, no_scene_df, X_train, X_test, y_train, y_test, train, test = custom_train_test_split(df, bag_of_bigrams, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any insights from sentiment score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>compound_score</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigger_scene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>0.443910</td>\n",
       "      <td>0.169071</td>\n",
       "      <td>0.387019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.422630</td>\n",
       "      <td>0.172477</td>\n",
       "      <td>0.404893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.431842</td>\n",
       "      <td>0.171002</td>\n",
       "      <td>0.397156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "compound_score         0         1         2\n",
       "trigger_scene                               \n",
       "False           0.443910  0.169071  0.387019\n",
       "True            0.422630  0.172477  0.404893\n",
       "All             0.431842  0.171002  0.397156"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.trigger_scene, df.compound_score, normalize='index', margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the scores across the different classes seems to be the about the same. Don't expect accuracy to increase much of at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "I will fit different models using different algorithms. I will also tune hyperparameters using randomized search cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mz211\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=123, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=25, n_jobs=4,\n",
       "          param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001B992ED01D0>, 'solver': ['newton-cg', 'lbfgs', 'saga', 'liblinear']},\n",
       "          pre_dispatch='2*n_jobs', random_state=123, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bigrams = LogisticRegression(random_state=123)\n",
    "parameters = {'C':sp.reciprocal(0.0001, 10000),\n",
    "              'solver':['newton-cg', 'lbfgs', 'saga', 'liblinear']}\n",
    "\n",
    "lr_bigrams_rs = RandomizedSearchCV(estimator=lr_bigrams, param_distributions=parameters, \n",
    "                                   n_jobs=4, random_state=123, verbose=3, n_iter=25)\n",
    "lr_bigrams_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 4.3406460764187536, 'solver': 'liblinear'}\n",
      "0.5897688145597639\n"
     ]
    }
   ],
   "source": [
    "print(lr_bigrams_rs.best_params_)\n",
    "print(lr_bigrams_rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mz211\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "          estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
       "            splitter='best'),\n",
       "          fit_params=None, iid='warn', n_iter=25, n_jobs=4,\n",
       "          param_distributions={'criterion': ['gini', 'entropy'], 'max_depth': [3, 14, 25], 'min_samples_split': [2, 50], 'min_samples_leaf': [1, 2, 3, 4, 5]},\n",
       "          pre_dispatch='2*n_jobs', random_state=123, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_bigrams = DecisionTreeClassifier(random_state=123)\n",
    "parameters = {'criterion':['gini', 'entropy'],\n",
    "              'max_depth':[int(x) for x in np.linspace(3, 25, 3)],\n",
    "              'min_samples_split':[int(x) for x in np.linspace(2,50, 2)],\n",
    "              'min_samples_leaf':[1, 2, 3, 4, 5]}\n",
    "\n",
    "dt_bigrams_rs = RandomizedSearchCV(estimator=dt_bigrams, param_distributions=parameters, \n",
    "                                   n_jobs=4, n_iter=25, random_state=123, verbose=3)\n",
    "dt_bigrams_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 2, 'min_samples_leaf': 3, 'max_depth': 25, 'criterion': 'gini'}\n",
      "0.5656665027053616\n"
     ]
    }
   ],
   "source": [
    "print(dt_bigrams_rs.best_params_)\n",
    "print(dt_bigrams_rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mz211\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 150 out of 150 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=123, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=4,\n",
       "          param_distributions={'n_estimators': [5, 16, 27, 38, 50], 'criterion': ['gini', 'entropy'], 'max_depth': [3, 16, 30], 'min_samples_split': [2, 20], 'min_samples_leaf': [1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=123, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_bigrams = RandomForestClassifier(random_state=123)\n",
    "parameters = {'n_estimators':[int(x) for x in np.linspace(5, 50, 5)],\n",
    "              'criterion':['gini', 'entropy'],\n",
    "              'max_depth':[int(x) for x in np.linspace(3, 30, 3)],\n",
    "              'min_samples_split':[int(x) for x in np.linspace(2, 20, 2)],\n",
    "              'min_samples_leaf':[int(x) for x in np.linspace(1, 3, 1)]}\n",
    "\n",
    "rf_bigrams_rs = RandomizedSearchCV(estimator=rf_bigrams, param_distributions=parameters, n_jobs=4, \n",
    "                           n_iter=50, random_state=123, verbose=3)\n",
    "rf_bigrams_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 30, 'criterion': 'entropy'}\n",
      "0.5725528775209051\n"
     ]
    }
   ],
   "source": [
    "print(rf_bigrams_rs.best_params_)\n",
    "print(rf_bigrams_rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mz211\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed: 24.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "          estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=4,\n",
       "          param_distributions={'n_neighbors': [3, 5, 7, 9, 13], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree', 'brute'], 'p': [1, 2], 'metric': ['minkowski', 'euclidean', 'manhattan']},\n",
       "          pre_dispatch='2*n_jobs', random_state=123, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knneighbors_bigrams = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':[3, 5, 7, 9, 13],\n",
    "              'weights':['uniform', 'distance'],\n",
    "              'algorithm':['ball_tree', 'kd_tree', 'brute'],\n",
    "              'p':[1, 2],\n",
    "              'metric':['minkowski', 'euclidean', 'manhattan']}\n",
    "\n",
    "knneighbors_bigrams_rs = RandomizedSearchCV(estimator=knneighbors_bigrams, param_distributions=parameters, n_jobs=4, \n",
    "                                    n_iter=10, verbose=3, random_state=123)\n",
    "knneighbors_bigrams_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': 'uniform', 'p': 1, 'n_neighbors': 13, 'metric': 'manhattan', 'algorithm': 'brute'}\n",
      "0.5459911460895229\n"
     ]
    }
   ],
   "source": [
    "print(knneighbors_bigrams_rs.best_params_)\n",
    "print(knneighbors_bigrams_rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mz211\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\mz211\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:271: UserWarning: The total space of parameters 24 is smaller than n_iter=25. Running 24 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=4)]: Done  72 out of  72 | elapsed: 17.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=123, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "          fit_params=None, iid='warn', n_iter=25, n_jobs=4,\n",
       "          param_distributions={'max_depth': [3, 13], 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators': [3, 15], 'gamma': [1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=123, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_bigrams = xgb.sklearn.XGBClassifier(random_state=123)\n",
    "parameters = {'max_depth':[int(x) for x in np.linspace(3, 13, 2)],\n",
    "              'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "              'n_estimators':[int(x) for x in np.linspace(3, 15, 2)],\n",
    "              'gamma':[int(x) for x in np.linspace(1, 11, 1)]}\n",
    "\n",
    "xgbc_bigrams_rs = RandomizedSearchCV(estimator=xgbc_bigrams, param_distributions=parameters, n_jobs=4, n_iter=25,\n",
    "                             verbose=3, random_state=123)\n",
    "xgbc_bigrams_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 15, 'max_depth': 13, 'learning_rate': 0.2, 'gamma': 1}\n",
      "0.559272011805214\n"
     ]
    }
   ],
   "source": [
    "print(xgbc_bigrams_rs.best_params_)\n",
    "print(xgbc_bigrams_rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression performed the best.\n",
    "0.58 accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual                    title  score\n",
       "0   False  spiderman_far_from_home    0.0\n",
       "1   False  spiderman_far_from_home    0.0\n",
       "2   False  spiderman_far_from_home    0.0\n",
       "3   False  spiderman_far_from_home    2.0\n",
       "4   False  spiderman_far_from_home    0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9872110181997049"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train.actual, lr_bigrams_rs.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>spiderman_far_from_home</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual                    title  score  predictions\n",
       "0   False  spiderman_far_from_home    0.0        False\n",
       "1   False  spiderman_far_from_home    0.0        False\n",
       "2   False  spiderman_far_from_home    0.0        False\n",
       "3   False  spiderman_far_from_home    2.0        False\n",
       "4   False  spiderman_far_from_home    0.0        False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['predictions'] = lr_bigrams_rs.predict(X_train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5870588235294117"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['predictions'] = lr_bigrams_rs.predict(X_test)\n",
    "accuracy_score(test.actual, test.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition of sentiment score did not improve accuracy.\n",
    "It actually hurt accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
